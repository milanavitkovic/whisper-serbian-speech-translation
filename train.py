# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GXU7otVCHx88zPdFSdTtV6846y76dmRH
"""

"""
Serbian ‚Üí English Speech Translation with Whisper

Fine-tuning OpenAI's Whisper model for direct speech-to-text translation
from Serbian audio to English text.

Training Data:
    - Ju≈æne Vesti (train split)
    - ParlaSpeech (train split)

Validation Data:
    - Ju≈æne Vesti (validation split)

Test Data:
    - Ju≈æne Vesti (test split)

Models Supported:
    - openai/whisper-small
    - openai/whisper-medium

Evaluation Metrics:
    - WER (Word Error Rate)
    - BLEU
    - METEOR
"""

import os
import torch
import nltk
from dataclasses import dataclass
from typing import Any, Dict, List, Union

from datasets import Dataset, Audio
from transformers import (
    WhisperProcessor,
    WhisperForConditionalGeneration,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments
)
from jiwer import wer

# Download required NLTK data
nltk.download("wordnet", quiet=True)
nltk.download("omw-1.4", quiet=True)


# ============================================================================
# CONFIGURATION
# ============================================================================

# Model configuration
# Options: "openai/whisper-small" or "openai/whisper-medium"
MODEL_NAME = "openai/whisper-small"

# Audio sampling rate (required by Whisper)
SAMPLE_RATE = 16000

# Output directories
OUTPUT_DIR = "./whisper-sr-en-finetuned"
FINAL_MODEL_DIR = "./whisper-sr-en-final"

# Data paths for Ju≈æne Vesti dataset
JUZNE_VESTI_PATHS = {
    # Serbian transcripts (format: audio_path\ttranscript)
    "sr_train": "data/juzne_vesti/train_sr.txt",
    "sr_val": "data/juzne_vesti/val_sr.txt",
    "sr_test": "data/juzne_vesti/test_sr.txt",

    # English translations (format: one translation per line)
    "en_train": "data/juzne_vesti/train_en.txt",
    "en_val": "data/juzne_vesti/val_en.txt",
    "en_test": "data/juzne_vesti/test_en.txt",

    # Audio directories
    "audio_train": "data/audio/juzne_vesti/train",
    "audio_val": "data/audio/juzne_vesti/val",
    "audio_test": "data/audio/juzne_vesti/test",
}

# Data paths for ParlaSpeech dataset
PARLA_SPEECH_PATHS = {
    # Serbian transcripts
    "sr_train": "data/parla_speech/train_sr.txt",

    # English translations
    "en_train": "data/parla_speech/train_en.txt",

    # Audio directory
    "audio_train": "data/audio/parla_speech/train",
}


# ============================================================================
# DATA LOADING FUNCTIONS
# ============================================================================

def load_parallel_data(
    sr_file: str,
    en_file: str,
    audio_base_path: str
) -> List[Dict[str, str]]:
    """
    Load parallel Serbian-English data and reconstruct audio paths.

    File Formats:
        - Serbian file: Each line contains 'audio_path\\ttranscript'
        - English file: Each line contains one translation
        - Files must have the same number of lines (aligned)

    Args:
        sr_file: Path to Serbian transcript file
        en_file: Path to English translation file
        audio_base_path: Base directory containing audio files

    Returns:
        List of dictionaries with 'audio', 'sr', and 'en' keys

    Raises:
        AssertionError: If Serbian and English files have different lengths
    """
    # Load Serbian transcripts
    with open(sr_file, "r", encoding="utf-8") as f:
        sr_lines = [line.strip().split("\t") for line in f if "\t" in line]

    # Load English translations
    with open(en_file, "r", encoding="utf-8") as f:
        en_lines = [line.strip() for line in f if line.strip()]

    # Verify alignment
    assert len(sr_lines) == len(en_lines), \
        f"Data mismatch: Serbian file has {len(sr_lines)} lines, English file has {len(en_lines)} lines"

    data = []
    for (sr_path, sr_text), en_text in zip(sr_lines, en_lines):
        # Extract filename from path (handles both Windows and Unix paths)
        filename = os.path.basename(sr_path).replace("\\", "/").split("/")[-1]

        # Construct full audio path
        audio_path = os.path.join(audio_base_path, filename)

        data.append({
            "audio": audio_path,
            "sr": sr_text,      # Serbian transcript
            "en": en_text       # English translation
        })

    return data


def prepare_dataset(
    data: List[Dict[str, str]],
    verify_audio: bool = True
) -> Dataset:
    """
    Prepare HuggingFace Dataset object from parallel data.

    This function:
    1. Optionally verifies that audio files exist
    2. Creates a HuggingFace Dataset object
    3. Casts audio column to required sampling rate (16kHz)

    Args:
        data: List of dicts with 'audio', 'sr', 'en' keys
        verify_audio: Whether to check if audio files exist

    Returns:
        HuggingFace Dataset with audio column cast to 16kHz
    """
    if verify_audio:
        print("üîç Verifying audio file existence...")
        valid_data = []
        missing_count = 0

        for d in data:
            if os.path.exists(d["audio"]):
                valid_data.append(d)
            else:
                missing_count += 1
                if missing_count <= 5:  # Show first 5 missing files
                    print(f"‚ö†Ô∏è  File not found: {d['audio']}")

        if missing_count > 0:
            print(f"‚ö†Ô∏è  Total missing files: {missing_count}")
            print(f"‚úÖ Valid files found: {len(valid_data)}")

        data = valid_data

    # Create HuggingFace Dataset
    dataset = Dataset.from_list([
        {
            "audio": d["audio"],
            "sentence": d["en"],    # English translation (target)
            "sr_text": d["sr"]      # Serbian transcript (optional reference)
        }
        for d in data
    ])

    # Cast audio column to Whisper's required sampling rate
    dataset = dataset.cast_column("audio", Audio(sampling_rate=SAMPLE_RATE))

    return dataset


# ============================================================================
# DATA COLLATOR
# ============================================================================

@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    """
    Data collator that handles batching for speech-to-text translation.

    Responsibilities:
        1. Process audio into mel-spectrogram features
        2. Tokenize target text (English translations)
        3. Pad inputs and labels to uniform length within each batch
        4. Mask padding tokens in labels (set to -100 for loss calculation)
    """
    processor: Any

    def __call__(
        self,
        features: List[Dict[str, Union[List[int], torch.Tensor]]]
    ) -> Dict[str, torch.Tensor]:
        """
        Process a batch of features.

        Args:
            features: List of feature dicts from the dataset

        Returns:
            Dictionary containing batched and padded tensors
        """
        # Extract and process audio into mel-spectrograms
        input_features = [
            {
                "input_features": self.processor(
                    f["audio"]["array"],
                    sampling_rate=SAMPLE_RATE
                ).input_features[0]
            }
            for f in features
        ]

        # Tokenize target sentences (English translations)
        label_features = [
            {"input_ids": self.processor.tokenizer(f["sentence"]).input_ids}
            for f in features
        ]

        # Pad input features to same length
        batch = self.processor.feature_extractor.pad(
            input_features,
            return_tensors="pt"
        )

        # Pad labels to same length
        labels_batch = self.processor.tokenizer.pad(
            label_features,
            return_tensors="pt"
        )

        # Replace padding tokens with -100 (ignored by loss function)
        labels = labels_batch["input_ids"].masked_fill(
            labels_batch.attention_mask.ne(1),
            -100
        )

        batch["labels"] = labels
        return batch


# ============================================================================
# EVALUATION METRICS
# ============================================================================

def compute_metrics(pred):
    """
    Compute evaluation metrics during training.

    This function calculates Word Error Rate (WER) between predictions
    and ground truth translations.

    Args:
        pred: Prediction output from the model

    Returns:
        Dictionary containing computed metrics
    """
    pred_ids = pred.predictions
    label_ids = pred.label_ids

    # Decode predictions
    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)

    # Decode labels (replace -100 with pad token for decoding)
    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id
    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)

    # Calculate WER (lower is better)
    wer_score = wer(label_str, pred_str)

    return {
        "wer": wer_score
    }


# ============================================================================
# MAIN TRAINING PIPELINE
# ============================================================================

if __name__ == "__main__":

    print("=" * 70)
    print("Serbian ‚Üí English Speech Translation - Whisper Fine-tuning")
    print("=" * 70)
    print(f"\nModel: {MODEL_NAME}")
    print(f"Output directory: {OUTPUT_DIR}")
    print(f"Final model directory: {FINAL_MODEL_DIR}")

    # ========================================================================
    # 1. LOAD DATA
    # ========================================================================

    print("\n" + "=" * 70)
    print("LOADING DATASETS")
    print("=" * 70)

    # Load Ju≈æne Vesti train set
    print("\nüì¶ Loading Ju≈æne Vesti train set...")
    jv_train_data = load_parallel_data(
        JUZNE_VESTI_PATHS["sr_train"],
        JUZNE_VESTI_PATHS["en_train"],
        JUZNE_VESTI_PATHS["audio_train"]
    )

    # Load ParlaSpeech train set
    print("üì¶ Loading ParlaSpeech train set...")
    ps_train_data = load_parallel_data(
        PARLA_SPEECH_PATHS["sr_train"],
        PARLA_SPEECH_PATHS["en_train"],
        PARLA_SPEECH_PATHS["audio_train"]
    )

    # Combine both datasets for training
    train_data = jv_train_data + ps_train_data
    print(f"‚úÖ Combined training data: {len(jv_train_data)} (JV) + {len(ps_train_data)} (PS) = {len(train_data)}")

    # Load Ju≈æne Vesti validation set
    print("\nüì¶ Loading Ju≈æne Vesti validation set...")
    val_data = load_parallel_data(
        JUZNE_VESTI_PATHS["sr_val"],
        JUZNE_VESTI_PATHS["en_val"],
        JUZNE_VESTI_PATHS["audio_val"]
    )

    # Load Ju≈æne Vesti test set
    print("üì¶ Loading Ju≈æne Vesti test set...")
    test_data = load_parallel_data(
        JUZNE_VESTI_PATHS["sr_test"],
        JUZNE_VESTI_PATHS["en_test"],
        JUZNE_VESTI_PATHS["audio_test"]
    )

    print(f"\nüìä Dataset Summary:")
    print(f"   Training:   {len(train_data):,} examples (JV + ParlaSpeech)")
    print(f"   Validation: {len(val_data):,} examples (JV only)")
    print(f"   Test:       {len(test_data):,} examples (JV only)")

    # ========================================================================
    # 2. PREPARE DATASETS
    # ========================================================================

    print("\n" + "=" * 70)
    print("PREPARING DATASETS")
    print("=" * 70)

    print("\nüîß Preparing training dataset...")
    train_ds = prepare_dataset(train_data, verify_audio=True)

    print("\nüîß Preparing validation dataset...")
    val_ds = prepare_dataset(val_data, verify_audio=True)

    print("\nüîß Preparing test dataset...")
    test_ds = prepare_dataset(test_data, verify_audio=True)

    print(f"\n‚úÖ Datasets ready!")
    print(f"   Train: {len(train_ds)} examples")
    print(f"   Val:   {len(val_ds)} examples")
    print(f"   Test:  {len(test_ds)} examples")

    # Show sample data
    print(f"\nüìã Sample from training set:")
    print(f"   Audio path: {train_ds[0]['audio']['path']}")
    print(f"   Serbian:    {train_ds[0]['sr_text'][:80]}...")
    print(f"   English:    {train_ds[0]['sentence'][:80]}...")

    # ========================================================================
    # 3. LOAD MODEL AND PROCESSOR
    # ========================================================================

    print("\n" + "=" * 70)
    print("LOADING MODEL")
    print("=" * 70)

    print(f"\nü§ñ Loading Whisper model: {MODEL_NAME}")
    processor = WhisperProcessor.from_pretrained(MODEL_NAME)
    model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME)

    # Configure model for Serbian ‚Üí English translation
    # This forces decoder to output English regardless of input language
    model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(
        language="english",   # Target language
        task="translate"      # Task: translate (not transcribe)
    )

    print(f"‚úÖ Model loaded: {model.num_parameters():,} parameters")
    print(f"‚úÖ Configured for: Serbian ‚Üí English translation")

    # ========================================================================
    # 4. INITIALIZE DATA COLLATOR
    # ========================================================================

    print("\nüîß Initializing data collator...")
    data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor)
    print("‚úÖ Data collator ready")

    # ========================================================================
    # 5. CONFIGURE TRAINING
    # ========================================================================

    print("\n" + "=" * 70)
    print("TRAINING CONFIGURATION")
    print("=" * 70)

    training_args = Seq2SeqTrainingArguments(
        output_dir=OUTPUT_DIR,

        # Batch configuration
        per_device_train_batch_size=8,           # Batch size per GPU
        per_device_eval_batch_size=8,            # Evaluation batch size
        gradient_accumulation_steps=2,           # Effective batch = 8 * 2 = 16

        # Optimization
        learning_rate=1e-5,                      # Learning rate
        warmup_ratio=0.1,                        # 10% warmup
        num_train_epochs=3,                      # Number of epochs

        # Memory optimization
        gradient_checkpointing=True,             # Save memory
        fp16=True,                               # Mixed precision training

        # Evaluation and saving
        evaluation_strategy="epoch",             # Evaluate each epoch
        save_strategy="epoch",                   # Save each epoch
        logging_strategy="epoch",                # Log each epoch

        # Generation settings
        predict_with_generate=True,              # Use generation for eval
        generation_max_length=225,               # Max generation length

        # Model selection
        load_best_model_at_end=True,             # Load best checkpoint
        metric_for_best_model="wer",             # Metric for selection
        greater_is_better=False,                 # Lower WER is better

        # Other settings
        remove_unused_columns=False,             # Keep all columns
        report_to=["tensorboard"]                # Logging
    )

    print(f"\nüìã Training Settings:")
    print(f"   Epochs:              {training_args.num_train_epochs}")
    print(f"   Batch size per GPU:  {training_args.per_device_train_batch_size}")
    print(f"   Gradient accum.:     {training_args.gradient_accumulation_steps}")
    print(f"   Effective batch:     {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}")
    print(f"   Learning rate:       {training_args.learning_rate}")
    print(f"   FP16:                {training_args.fp16}")
    print(f"   Gradient checkpoint: {training_args.gradient_checkpointing}")

    # ========================================================================
    # 6. INITIALIZE TRAINER
    # ========================================================================

    print("\nüîß Initializing trainer...")
    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=train_ds,
        eval_dataset=val_ds,
        data_collator=data_collator,
        tokenizer=processor.feature_extractor,
        compute_metrics=compute_metrics
    )
    print("‚úÖ Trainer ready")

    # ========================================================================
    # 7. START TRAINING
    # ========================================================================

    print("\n" + "=" * 70)
    print("STARTING TRAINING")
    print("=" * 70)
    print("\nüöÄ Training started...\n")

    # Train the model
    trainer.train()

    print("\n" + "=" * 70)
    print("‚úÖ TRAINING COMPLETE")
    print("=" * 70)

    # ========================================================================
    # 8. SAVE MODEL
    # ========================================================================

    print(f"\nüíæ Saving final model to: {FINAL_MODEL_DIR}")
    trainer.save_model(FINAL_MODEL_DIR)
    processor.save_pretrained(FINAL_MODEL_DIR)
    print(f"‚úÖ Model and processor saved successfully!")

    # ========================================================================
    # 9. QUICK INFERENCE TEST
    # ========================================================================

    print("\n" + "=" * 70)
    print("QUICK INFERENCE TEST")
    print("=" * 70)

    print("\nüß™ Testing model on a sample from test set...\n")

    # Get a test sample
    sample = test_ds[0]
    audio_input = sample["audio"]["array"]

    # Process audio
    input_features = processor(
        audio_input,
        sampling_rate=SAMPLE_RATE,
        return_tensors="pt"
    ).input_features

    # Move to GPU if available
    if torch.cuda.is_available():
        input_features = input_features.to("cuda")
        model = model.to("cuda")

    # Generate translation
    with torch.no_grad():
        predicted_ids = model.generate(input_features)

    # Decode prediction
    prediction = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]

    # Display results
    print(f"Serbian (original):")
    print(f"  {sample['sr_text'][:150]}...\n")
    print(f"English (reference):")
    print(f"  {sample['sentence'][:150]}...\n")
    print(f"English (predicted):")
    print(f"  {prediction[:150]}...\n")

    # ========================================================================
    # DONE
    # ========================================================================

    print("=" * 70)
    print("‚úÖ ALL DONE!")
    print("=" * 70)
    print(f"\nModel saved at: {FINAL_MODEL_DIR}")
    print(f"Training logs at: {OUTPUT_DIR}")
    print("\nNext steps:")
    print("  1. Evaluate on test set (compute WER, BLEU, METEOR)")
    print("  2. Compare with baseline model")
    print("  3. Try Whisper-medium for comparison")
    print("\nTo monitor training:")
    print(f"  tensorboard --logdir={OUTPUT_DIR}")
    print("=" * 70)