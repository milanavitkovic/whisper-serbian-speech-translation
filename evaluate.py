# -*- coding: utf-8 -*-
"""evaluate.ipynb

Automatically generated by Colab.
"""

"""
Serbian ‚Üí English Speech Translation Evaluation Script

This script evaluates fine-tuned Whisper models on Serbian-to-English
speech translation task using three key metrics:
- WER (Word Error Rate)
- BLEU Score
- METEOR Score

Usage:
    python evaluate.py --model_path <path_to_model> --test_data <path_to_test_folder>
"""

import os
import argparse
import warnings
import torch
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from jiwer import wer
import nltk
from nltk.translate.meteor_score import meteor_score
from nltk.tokenize import word_tokenize
from sacrebleu.metrics import BLEU
from tqdm import tqdm
from datasets import Dataset
from transformers import WhisperProcessor, WhisperForConditionalGeneration
import librosa

warnings.filterwarnings('ignore')


# ============================================================================
# CONFIGURATION
# ============================================================================

DEFAULT_MODEL_PATH = "./whisper-sr-en-final"
DEFAULT_TEST_SR = "data/juzne_vesti/test_sr.txt"
DEFAULT_TEST_EN = "data/juzne_vesti/test_en.txt"
DEFAULT_AUDIO_FOLDER = "data/audio/juzne_vesti/test"

# Audio configuration
SAMPLE_RATE = 16000


# ============================================================================
# NLTK SETUP
# ============================================================================

def setup_nltk():
    """Download required NLTK resources for METEOR"""
    required_nltk_data = ['punkt_tab', 'punkt', 'wordnet', 'omw-1.4']

    for resource in required_nltk_data:
        try:
            if resource == 'punkt_tab':
                nltk.data.find('tokenizers/punkt_tab')
            elif resource == 'punkt':
                nltk.data.find('tokenizers/punkt')
            elif resource == 'wordnet':
                nltk.data.find('corpora/wordnet')
            elif resource == 'omw-1.4':
                nltk.data.find('corpora/omw-1.4')
        except LookupError:
            print(f"üì• Downloading {resource}...")
            nltk.download(resource, quiet=True)

    print("‚úÖ All NLTK resources ready!")


# ============================================================================
# DATA LOADING
# ============================================================================

def load_test_data(
    sr_file: str,
    en_file: str,
    audio_base_path: str
) -> Dataset:
    """
    Load test data from Serbian and English transcript files.

    File Formats:
        - Serbian file: Each line contains 'audio_path\\ttranscript'
        - English file: Each line contains one translation
        - Files must have the same number of lines (aligned)

    Args:
        sr_file: Path to Serbian transcript file
        en_file: Path to English translation file
        audio_base_path: Base directory containing audio files

    Returns:
        HuggingFace Dataset with audio paths and translations
    """
    print(f"üìÇ Loading Serbian transcripts from: {sr_file}")
    print(f"üìÇ Loading English translations from: {en_file}")
    print(f"üìÅ Audio folder: {audio_base_path}")

    # Load Serbian transcripts (audio_path\ttranscript format)
    with open(sr_file, 'r', encoding='utf-8') as f:
        sr_lines = [line.strip().split('\t') for line in f if '\t' in line]

    # Load English translations
    with open(en_file, 'r', encoding='utf-8') as f:
        en_lines = [line.strip() for line in f if line.strip()]

    # Verify alignment
    assert len(sr_lines) == len(en_lines), \
        f"Mismatch: Serbian file has {len(sr_lines)} lines, English file has {len(en_lines)} lines"

    # Extract audio paths and construct full paths
    audio_paths = []
    for sr_path, sr_text in sr_lines:
        # Extract filename from path (handles both Windows and Unix paths)
        filename = os.path.basename(sr_path).replace("\\", "/").split("/")[-1]

        # Construct full audio path
        full_path = os.path.join(audio_base_path, filename)
        audio_paths.append(full_path)

    # Check for missing files
    missing = [p for p in audio_paths if not os.path.exists(p)]
    if missing:
        print(f"‚ö†Ô∏è  Missing {len(missing)} files")
        for m in missing[:3]:
            print(f"   - {m}")
        raise FileNotFoundError("Some audio files are missing - check paths")

    # Create dataset
    dataset = Dataset.from_dict({
        "audio_path": audio_paths,
        "sentence": en_lines
    })

    print(f"‚úÖ Dataset ready ‚Äì {len(dataset)} examples")
    return dataset


# ============================================================================
# METRICS CALCULATION
# ============================================================================

def calculate_overall_metrics(predictions: list, references: list) -> dict:
    """
    Calculate overall corpus-level metrics: WER, BLEU, METEOR.

    Args:
        predictions: List of predicted translations
        references: List of reference translations

    Returns:
        Dictionary containing WER, BLEU, and METEOR scores
    """
    print("\n" + "="*80)
    print("CALCULATING METRICS")
    print("="*80)

    metrics = {}

    # Word Error Rate (WER)
    try:
        wer_score = wer(references, predictions)
        metrics['WER'] = wer_score * 100
        print(f"WER (Word Error Rate):       {wer_score*100:.2f}%")
    except Exception as e:
        print(f"‚ö†Ô∏è  Error calculating WER: {e}")
        metrics['WER'] = None

    # BLEU Score
    try:
        bleu = BLEU()
        bleu_score = bleu.corpus_score(predictions, [references])
        metrics['BLEU'] = bleu_score.score
        print(f"BLEU Score:                  {bleu_score.score:.2f}")
    except Exception as e:
        print(f"‚ö†Ô∏è  Error calculating BLEU: {e}")
        metrics['BLEU'] = None

    # METEOR Score
    try:
        import evaluate
        meteor_metric = evaluate.load('meteor')
        meteor_result = meteor_metric.compute(predictions=predictions, references=references)
        metrics['METEOR'] = meteor_result['meteor'] * 100
        print(f"METEOR Score:                {meteor_result['meteor']*100:.2f}%")
    except Exception:
        # Fallback to NLTK implementation
        try:
            meteor_scores = []
            for ref, pred in zip(references, predictions):
                try:
                    ref_tokens = word_tokenize(ref.lower())
                    pred_tokens = word_tokenize(pred.lower())
                    score = meteor_score([ref_tokens], pred_tokens)
                    meteor_scores.append(score)
                except:
                    continue

            if meteor_scores:
                avg_meteor = sum(meteor_scores) / len(meteor_scores) * 100
                metrics['METEOR'] = avg_meteor
                print(f"METEOR Score (NLTK):         {avg_meteor:.2f}%")
            else:
                metrics['METEOR'] = None
                print(f"‚ö†Ô∏è  Could not calculate METEOR")
        except Exception as e2:
            print(f"‚ö†Ô∏è  METEOR calculation failed: {e2}")
            metrics['METEOR'] = None

    print("="*80)
    return metrics


def calculate_per_sample_metrics(reference: str, prediction: str) -> dict:
    """
    Calculate metrics for a single example: WER, BLEU, METEOR.

    Args:
        reference: Reference translation
        prediction: Predicted translation

    Returns:
        Dictionary with per-sample WER, BLEU, and METEOR
    """
    metrics = {}

    # WER
    try:
        metrics['WER'] = wer([reference], [prediction]) * 100
    except:
        metrics['WER'] = None

    # BLEU
    try:
        bleu = BLEU()
        metrics['BLEU'] = bleu.sentence_score(prediction, [reference]).score
    except:
        metrics['BLEU'] = None

    # METEOR
    try:
        import evaluate
        meteor_metric = evaluate.load('meteor')
        result = meteor_metric.compute(predictions=[prediction], references=[reference])
        metrics['METEOR'] = result['meteor'] * 100
    except:
        try:
            ref_tokens = word_tokenize(reference.lower())
            pred_tokens = word_tokenize(prediction.lower())
            metrics['METEOR'] = meteor_score([ref_tokens], pred_tokens) * 100
        except:
            metrics['METEOR'] = None

    return metrics


# ============================================================================
# MODEL EVALUATION
# ============================================================================

def evaluate_model(
    model,
    processor,
    dataset: Dataset,
    device: str,
    num_samples: int = None
) -> dict:
    """
    Evaluate model on test dataset.

    Args:
        model: Fine-tuned Whisper model
        processor: Whisper processor
        dataset: Test dataset
        device: Device to run inference on ('cuda' or 'cpu')
        num_samples: Number of samples to evaluate (None = all)

    Returns:
        Dictionary with evaluation results
    """
    model.eval()
    predictions, references, audio_paths = [], [], []

    num_samples = num_samples or len(dataset)
    print(f"\nüîç Evaluating on {num_samples} examples...")

    for i in tqdm(range(num_samples)):
        try:
            sample = dataset[i]

            # Load audio
            audio_array, sr = librosa.load(sample["audio_path"], sr=SAMPLE_RATE)

            # Process audio
            input_features = processor(
                audio_array,
                sampling_rate=sr,
                return_tensors="pt"
            ).input_features.to(device)

            # Configure for Serbian ‚Üí English translation
            forced_decoder_ids = processor.get_decoder_prompt_ids(
                language="sr",
                task="translate"
            )

            # Generate translation
            with torch.no_grad():
                pred_ids = model.generate(
                    input_features,
                    max_length=225,
                    forced_decoder_ids=forced_decoder_ids
                )

            # Decode prediction
            pred_eng = processor.batch_decode(pred_ids, skip_special_tokens=True)[0]
            ref_eng = sample["sentence"]

            predictions.append(pred_eng.strip())
            references.append(ref_eng.strip())
            audio_paths.append(sample["audio_path"])

        except Exception as e:
            print(f"‚ö†Ô∏è  Error on example {i}: {e}")
            continue

    if not predictions:
        raise RuntimeError("No successful predictions - check audio files and librosa")

    # Calculate overall metrics
    overall_metrics = calculate_overall_metrics(predictions, references)

    # Calculate per-sample metrics
    print("\nüìä Calculating per-sample metrics...")
    per_sample_metrics = []
    for pred, ref in tqdm(zip(predictions, references), total=len(predictions)):
        per_sample_metrics.append(calculate_per_sample_metrics(ref, pred))

    # Create results dataframe
    results_df = pd.DataFrame({
        "audio_path": audio_paths,
        "reference_english": references,
        "prediction_english": predictions,
    })

    # Add per-sample metrics
    for metric in ['WER', 'BLEU', 'METEOR']:
        results_df[metric] = [m[metric] for m in per_sample_metrics]

    # Add length difference
    results_df["length_diff"] = (
        results_df["prediction_english"].str.len() -
        results_df["reference_english"].str.len()
    )

    # Save results
    output_file = "evaluation_results.csv"
    results_df.to_csv(output_file, index=False, encoding="utf-8-sig")
    print(f"\nüíæ Detailed results saved to: {output_file}")

    # Print summary
    print(f"\n" + "=" * 60)
    print(f"‚úÖ EVALUATION SUMMARY")
    print(f"=" * 60)
    for metric, value in overall_metrics.items():
        if value is not None:
            print(f"{metric:8}: {value:.2f}{'%' if metric != 'BLEU' else ''}")
    print(f"Examples: {len(predictions)}")
    print(f"=" * 60)

    return {
        "overall_metrics": overall_metrics,
        "df": results_df
    }


# ============================================================================
# RESULTS ANALYSIS
# ============================================================================

def analyze_results(results_df: pd.DataFrame):
    """
    Analyze evaluation results in detail.

    Args:
        results_df: DataFrame with evaluation results
    """
    print("\n" + "="*60)
    print("üîç DETAILED RESULTS ANALYSIS")
    print("="*60)

    # Top 5 best examples (by BLEU)
    print("\n‚úÖ TOP 5 BEST EXAMPLES (highest BLEU):")
    top_5 = results_df.nlargest(5, 'BLEU')[
        ['BLEU', 'METEOR', 'WER', 'reference_english', 'prediction_english']
    ]
    for idx, row in top_5.iterrows():
        print(f"\nBLEU: {row['BLEU']:.2f} | METEOR: {row['METEOR']:.2f} | WER: {row['WER']:.2f}%")
        print(f"REF:  {row['reference_english'][:80]}...")
        print(f"PRED: {row['prediction_english'][:80]}...")

    # Top 5 worst examples (by BLEU)
    print("\n\n‚ùå TOP 5 WORST EXAMPLES (lowest BLEU):")
    worst_5 = results_df.nsmallest(5, 'BLEU')[
        ['BLEU', 'METEOR', 'WER', 'reference_english', 'prediction_english']
    ]
    for idx, row in worst_5.iterrows():
        print(f"\nBLEU: {row['BLEU']:.2f} | METEOR: {row['METEOR']:.2f} | WER: {row['WER']:.2f}%")
        print(f"REF:  {row['reference_english'][:80]}...")
        print(f"PRED: {row['prediction_english'][:80]}...")

    # Statistics
    print("\n\nüìä STATISTICS:")
    for metric in ['BLEU', 'METEOR', 'WER']:
        if metric in results_df.columns:
            valid_data = results_df[metric].dropna()
            if len(valid_data) > 0:
                print(f"{metric:8} - Min: {valid_data.min():.2f}, "
                      f"Max: {valid_data.max():.2f}, "
                      f"Mean: {valid_data.mean():.2f}, "
                      f"Std: {valid_data.std():.2f}")

    # Correlations
    print("\nüìà METRIC CORRELATIONS:")
    corr_metrics = ['BLEU', 'METEOR', 'WER']
    corr_matrix = results_df[corr_metrics].corr()
    print(corr_matrix.round(3))

    print("="*60)


# ============================================================================
# VISUALIZATION
# ============================================================================

def plot_results(results_df: pd.DataFrame, output_file: str = "evaluation_plots.png"):
    """
    Create visualization plots for evaluation results.

    Args:
        results_df: DataFrame with evaluation results
        output_file: Path to save the plot
    """
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))

    # BLEU distribution
    axes[0, 0].hist(results_df["BLEU"].dropna(), bins=30, color='skyblue', edgecolor='black')
    axes[0, 0].axvline(results_df["BLEU"].mean(), color='red', linestyle='--',
                       label=f'Mean: {results_df["BLEU"].mean():.2f}')
    axes[0, 0].set_title("BLEU Score Distribution", fontsize=14, fontweight='bold')
    axes[0, 0].set_xlabel("BLEU Score")
    axes[0, 0].set_ylabel("Count")
    axes[0, 0].legend()
    axes[0, 0].grid(alpha=0.3)

    # METEOR distribution
    axes[0, 1].hist(results_df["METEOR"].dropna(), bins=30, color='lightgreen', edgecolor='black')
    axes[0, 1].axvline(results_df["METEOR"].mean(), color='red', linestyle='--',
                       label=f'Mean: {results_df["METEOR"].mean():.2f}')
    axes[0, 1].set_title("METEOR Score Distribution", fontsize=14, fontweight='bold')
    axes[0, 1].set_xlabel("METEOR Score")
    axes[0, 1].set_ylabel("Count")
    axes[0, 1].legend()
    axes[0, 1].grid(alpha=0.3)

    # WER distribution
    axes[1, 0].hist(results_df["WER"].dropna(), bins=30, color='salmon', edgecolor='black')
    axes[1, 0].axvline(results_df["WER"].mean(), color='red', linestyle='--',
                       label=f'Mean: {results_df["WER"].mean():.2f}%')
    axes[1, 0].set_title("WER Distribution", fontsize=14, fontweight='bold')
    axes[1, 0].set_xlabel("WER Score (%)")
    axes[1, 0].set_ylabel("Count")
    axes[1, 0].legend()
    axes[1, 0].grid(alpha=0.3)

    # BLEU vs WER scatter
    axes[1, 1].scatter(results_df["BLEU"], results_df["WER"], alpha=0.5, s=20, color='orange')
    axes[1, 1].set_title("BLEU vs WER", fontsize=14, fontweight='bold')
    axes[1, 1].set_xlabel("BLEU Score")
    axes[1, 1].set_ylabel("WER Score (%)")
    axes[1, 1].grid(alpha=0.3)

    plt.tight_layout()
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"\nüìä Plots saved to: {output_file}")
    plt.show()


# ============================================================================
# MAIN FUNCTION
# ============================================================================

def main():
    """Main evaluation pipeline"""

    # Parse command-line arguments
    parser = argparse.ArgumentParser(description="Evaluate Whisper Serbian‚ÜíEnglish translation")
    parser.add_argument("--model_path", type=str, default=DEFAULT_MODEL_PATH,
                       help="Path to fine-tuned model")
    parser.add_argument("--sr_file", type=str, default=DEFAULT_TEST_SR,
                       help="Path to Serbian transcript file (audio_path\\ttranscript format)")
    parser.add_argument("--en_file", type=str, default=DEFAULT_TEST_EN,
                       help="Path to English translation file")
    parser.add_argument("--audio_folder", type=str, default=DEFAULT_AUDIO_FOLDER,
                       help="Base directory containing audio files")
    parser.add_argument("--num_samples", type=int, default=None,
                       help="Number of samples to evaluate (default: all)")
    parser.add_argument("--output_dir", type=str, default=".",
                       help="Directory to save results")

    args = parser.parse_args()

    # Setup
    print("=" * 70)
    print("Serbian ‚Üí English Speech Translation - Evaluation")
    print("=" * 70)

    setup_nltk()

    # Device configuration
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"\nüì¶ Loading model on {device}...")
    print(f"Model path: {args.model_path}")

    # Load model and processor
    processor = WhisperProcessor.from_pretrained(args.model_path)
    model = WhisperForConditionalGeneration.from_pretrained(args.model_path).to(device)
    print("‚úÖ Model loaded successfully")

    # Load test data
    test_dataset = load_test_data(
        sr_file=args.sr_file,
        en_file=args.en_file,
        audio_base_path=args.audio_folder
    )

    # Evaluate
    eval_results = evaluate_model(
        model=model,
        processor=processor,
        dataset=test_dataset,
        device=device,
        num_samples=args.num_samples
    )

    # Analyze results
    analyze_results(eval_results["df"])

    # Create visualizations
    plot_output = os.path.join(args.output_dir, "evaluation_plots.png")
    plot_results(eval_results["df"], output_file=plot_output)

    print("\n‚úÖ Evaluation complete!")
    print(f"üìÅ Results: {os.path.join(args.output_dir, 'evaluation_results.csv')}")
    print(f"üìä Plots: {plot_output}")


if __name__ == "__main__":
    main()
